{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.8.7 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gather a large number of tweets from a specified period. Only from a week ago due to endpoint restrictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "ENDPOINT = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "source": [
    "## Function that interfaces with Twitter's recent tweets endpoint and pulls tweets until `num_tweets` limit is hit or when it runs out pages to paginate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_tweets(keywords, start, end, num_tweets=1000):\n",
    "\n",
    "    token = open('bearer_token.txt', 'r').read()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    tweet_fields = \"id,created_at,author_id\"\n",
    "    max_results = 100 # Tweets per request.\n",
    "    tweet_constraints = \" \".join([\n",
    "        str(keywords),\n",
    "        \"-is:retweet\",\n",
    "        \"-has:media\",\n",
    "        \"-has:images\",\n",
    "        \"-has:links\",\n",
    "        \"-is:reply\",\n",
    "        \"lang:en\"\n",
    "    ])\n",
    "\n",
    "    tweets_df = pd.DataFrame()\n",
    "    \n",
    "    next = None\n",
    "\n",
    "    while tweets_df.shape[0] < num_tweets and next != -1:\n",
    "\n",
    "        request_parameters = \"&\".join([\n",
    "            f\"query={tweet_constraints}\",\n",
    "            f\"tweet.fields={tweet_fields}\",\n",
    "            f\"max_results={max_results}\",\n",
    "            f\"start_time={start.isoformat()}Z\",\n",
    "            f\"end_time={end.isoformat()}Z\"\n",
    "        ])\n",
    "\n",
    "        # Seeing whether to go to the next page.\n",
    "        if next is not None:\n",
    "            request_parameters += f\"&next_token={next}\"\n",
    "\n",
    "        # Complete the URL.\n",
    "        url = f\"{ENDPOINT}?{request_parameters}\"\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "\n",
    "        response_json = response.json()\n",
    "\n",
    "        # Checking if the next_token exists to move onto more tweets.\n",
    "        next = response_json['meta']['next_token'] \\\n",
    "            if 'next_token' in response_json['meta'].keys() else -1\n",
    "\n",
    "        data = response_json['data']\n",
    "\n",
    "        tweets_df = tweets_df.append(data, ignore_index=True)\n",
    "\n",
    "    return tweets_df"
   ]
  },
  {
   "source": [
    "## Playground"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = gather_tweets(\"millennials\", dt.datetime(2021, 4, 15), dt.datetime(2021, 4, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df, open('tweets/coffee.pkl', 'wb'))"
   ]
  }
 ]
}